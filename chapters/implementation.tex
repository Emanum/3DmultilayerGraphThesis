\chapter{Implementation}
\label{chap:Impl}
\section{Programming APIs}

To implement VR applications, we can work directly with the SDK provided from each headset manufacturer, the advantage is that we can interact at a low level with the devices.\\ 
Another way is to use an SDK of a well-defined standard for example OpenVR from Valve or OpenXR from the Khronos group. This allows us to write applications for a target group of devices like 6-DOF supporting headsets without having to deal with different manufacturer SDKs (see Figure \ref{fig:openxr-overview}). On a side note: the X in OpenXR means that the specification is not only used for virtual reality (VR) applications but also for augmented reality (AR) and other technologies (XR) possibly proposed in the future.\\
Lastly, we can work with different engines which will provide with additional features. While classical desktop engines rely on downloading a complete bundled software package for distributing the application, WebXR and WebVR allow us to distribute and run our application via the browser, similar to WebGL.

At the time of writing, the OpenXR and WebXR specification is still relatively new being in its first revision therefore it is not implemented by each headset manufacturer, browser and engine yet. However, these standards are meant to replace the old OpenVR and WebVR in the future.

\begin{figure}[!hbt]
    \centering
    \includegraphics[width=\textwidth]{graphics/openXR-overview.jpg}
    \caption{Overview of the OpenXR API Stack \cite{khronosGroupOpenXR}}
    \label{fig:openxr-overview}
\end{figure}

\section{Prior Publication}
For our implementation we extend the code of a prior publication from Sorger et al. \cite{sorger_immersive_2019}. 
They implemented an immersive visualization for node-link graphs without hierarchical information. Their implementation already contains a common force-based layout, the laser-pointer ray casting technique to select objects, a free flying as well as animated teleport navigation method and lastly a code structure with an included render engine. A screenshot of their work is shown in Figure \ref{fig:priorPublication}.\\
We extended their implementation by adding the ability to visualization hierarchical networks as described in Section \ref{chap:proposed-Solution}. To achieve that we had to adapt the layout algorithm, add transparent rendering, adapt some visual rendering aspects, change the target position for the animated teleport, change the controller button mappings, implement the link filtering technique, add automatic/manual scaling of the virtual scene and automatic/manual adaption of the free flying speed.
This process is described in detail in Section \ref{sec:applOverview} and \ref{sec:applDetails}.

\begin{figure}[!hbt]
    \centering
    \includegraphics[width=\textwidth]{graphics/screenshotPriorPublication.jpg}
    \caption{Screenshot of the visualization from Sorger et al. \cite{sorger_immersive_2019} we extend our implementation on.}
    \label{fig:priorPublication}
\end{figure}

\section{Technology}

The application is implemented in plain JavaScript and runs in browsers that support the WebVR standard. 
At the time of writing this only applies to Firefox for Windows, our tested version is 85.0.2 (64-Bit). 
WebVR is already deprecated and being replaced with the new WebXR standard, therefore it is unlikely that other browser vendors will implement the standard in the future.\\
In order to reduce the complexity of the rendering and using the WebVR standard the application uses the framework A-Frame \cite{aframe}. A-Frame internally uses three.js \cite{threejs} as a render engine. The code from the publication we extended uses A-Frame in version 0.9.2 from September 2019, this version uses three.js version 0.108.0. 
The old version of A-Frame brings the limitation of only supporting WebVR because WebXR was not yet ready back in 2019. 
Updating A-Frame to a current version that supports WebXR was not possible within a reasonable time therefore we decided to stick to the old version and only support WebVR browsers and headsets.
The primary VR headset we are targeting with our application is the original HTC Vive.

\section{Application Overview}




\label{sec:applOverview}
\subsection{Data Structure}
\label{subSec:dataStruct}
To get a better understanding of the implementation we begin with the data structure for storing the graph.
Listing \ref{lst:internalJSON} shows a minimalistic example of the data structure that we use an input data source. The data is formatted as a JSON object with a flat list of nodes and link. The hierarchical information is encoded with the attributes childNodesIDs and parentNodeID as references. 
In addition to the necessary attributes, each node and link object can have additional attributes depending on the specific dataset.

\begin{lstlisting}[language=json,label={lst:internalJSON},caption=minimal JSON input data structure]
{
    "nodes": [
        {
            "id": "0.0",
            "weight": 4.5,
            "color": "rgb(77, 175, 74)",
            "layer": "0",
            "desc": "0.0",
            "childNodeIDs": [
                "1.1",
                "1.3",
                ...
            ]
        },
        ...
        {
            "id": "1.1",
            "weight": 1.7,
            "color": "rgb(250, 250, 110)",
            "layer": "1",
            "desc": "1.1",
            "parentNodeID": "0.0",
            "childNodeIDs": [
                "2.1",
                ...
            ]
        },
        ...
    ],
    "links": [
        {
            "color": "rgb(77, 175, 74)",
            "layer": "0",
            "source": "0.0",
            "target": "0.1",
            "label": "0.0 - 0.1",
            "linkwidth": 0.35
        },
        ...
    ]
}
\end{lstlisting}

\subsection{Program Flow}
\label{section:programFlow}
\begin{figure}[!hbt]
    \centering
    \includegraphics[width=0.72\textwidth]{graphics/vrgraph_flow.jpg}
    \caption{Sequence diagram of the program flow for our application.}
    \label{fig:impl_programFlow}
\end{figure}

We separated the application into multiple processing steps. Figure \ref{fig:impl_programFlow} describes the program flow and all abstract steps that are necessary for the visualization to work.
It contains of an offline preparation phase where the graph data is converted once to our internal JSON data structure (see Section \ref{sec:preprocessing}) and a runtime phase which is executed every time the browser loads the webpage.
After loading the data preparations for the instanced rendering (see Section \ref{sec:rendering}) and the force based layout (see Section \ref{sec:layoutCalculation}) are done.
Then the main render loop is executed which produces a rendered frame for each interaction. In addition, it handles the layout calculation (see Section \ref{sec:layoutCalculation}), navigation methods (see Section \ref{sec:vrInteractions}), scaling (see Section \ref{sec:scaling}), filtering the graph's links (see Section \ref{sec:linkFiltering}) and performing the ray casting for the virtual laser pointer.
The objects that got intersected by the virtual laser pointer are used in the navigation and interaction techniques. 
Details on how intersected objects are determined by ray casting is not further described as this method is already provided from the original implementation. 

\subsection{Virtual Scene Graph}
A-Frame application are build by creating a virtual scene graph.
It uses an entity-component-system architecture which follows the  composition over inheritance and hierarchy principle. 
This means that every object in A-Frame is an entity that can be customized by code.
There are various components that can be reused and extended, the base component is represented by the <a-entity> element.\\
Listing \ref{lst:virtualSceneGraph} shows a simplified version of the virtual scene graph the application uses. 
It consists of entities for both Vive controllers, a passive and active camera and rig setup and the graph object itself.
Most data and logic is encapsulated in the graphData object. We maintain here two lists of data: a list of nodes/links for the ray casting entities and a list of nodes/links for the actual rendered entities. 
The reason for this duplicated data is that we use  instanced objects for rendering (see Section \ref{sec:rendering}) and these can not be used for ray casting directly.

\begin{lstlisting}[label={lst:virtualSceneGraph},caption=Simplified virtual A-Frame scene graph used by the application.]
<body>
    <a-scene ... >
        <a-entity vive-controls="hand: left"> </a-entity>
        <a-entity vive-controls="hand: right"> </a-entity>
        
        <a-entity id="graphData" json-url="data/inputData.json" ... >
            <a-entity id="passive_rig"  ... >  
                <a-entity id="passive_cam" ... > </a-entity>
            </a-entity>
            ...
        </a-entity>

        <a-entity id="active_rig" ... >
            <a-text id="controllerLabel" ... > </a-text>
            <a-entity id="active_cam" ... > </a-entity>
        </a-entity>
        ...
    </a-scene>
</body>
\end{lstlisting}

\section{Application Details}
\label{sec:applDetails}
\subsection{Preprocessing Scripts}
\label{sec:preprocessing}

The visualization expects a combined JSON file with all nodes and links and their assigned layer as seen in Section \ref{subSec:dataStruct}.
A common data export format for networks are multiple CSV files, one file for the nodes and another for their edges.
In a hierarchical or clustered network there is an additional file with the hierarchical mapping.
Therefore, we needed a script to convert these CSV files to our JSON data format. For convenience most parameters can be configured by an additional config file without changing any program code. 
While transforming the data structure, all node IDs are extended with their hierarchical layer information to ensure unique IDs for the entire dataset.  
In addition, we also normalize the node and link width and add color attributes for a simplified rendering process later on.\\
To test our visualization with various networks of different sizes there is also a script for generating test data available.

\subsection{Automated force layout}
\label{sec:layoutCalculation}

The node layout is automatically calculated on runtime by a force system powered by D3-Force. At the beginning all D3-Force specific parameters including alphaDecay, alphaTarget, alphaMin, velocityDecay and cooldownTime are set. 
These parameters adjust the flexibility of the simulation by changing the strength behavior for all forces, how much the velocity is decreased between every simulation step and how many simulation steps are applied in total.
In a second step we add all forces to the simulation, this process is shown in Listing \ref{lst:addForces}.\\
The center, many-body, link and collision force use the classical implementation from D3 force slightly adapted. 
For implementation details of the spherical constraint see Listing \ref{lst:sphericalConstraint}. The main goal here is to add a velocity for each child node depending on the current node position.\\
Lastly a simulation steps is called every frame which handles the subroutine calls for the different forces. The simulation is then stopped when a fixed number of steps is done determined by the alpha parameters set in the first step.

\begin{lstlisting}[language=JavaScript,label={lst:addForces},caption=Simplified algorithm that shows which forces are added to the simulation.] 
//add forces for hierarchical layer 0
d3ForceLayout.force("layer0-center",centerForce(...));
d3ForceLayout.force("layer0-link",linkForce(...))
d3ForceLayout.force("layer0-manyBody",manyBodyForce(...));
d3ForceLayout.force("layer0-collide",collideForce(...)));

//add forces for each parent node (layer 1-n)
parentNodeIDs.forEach((parentNodeID, index) => {
    const filteredNodesForLayer = filterNodes(nodes, function (d) {
        return d[parentNodeIDKey] === parentNodeID && parseInt(d.layer) === childLayer;
    })

    d3ForceLayout.force(parentNodeID + "-sphericalConstraint",sphericalConstraint(filteredNodesForLayer,...));
    d3ForceLayout.force(parentNodeID + "-forceCollision",collisionForce(filteredNodesForLayer,...));
    d3ForceLayout.force(parentNodeID+ "-manyBody",manyBodyForce(filteredNodesForLayer,...));
    d3ForceLayout.force(parentNodeID+ "-link",linkForce(filteredNodesForLayer,...));
});
\end{lstlisting}
\begin{lstlisting}[language=JavaScript,label={lst:sphericalConstraint},caption=Simplified algorithm for the spherical constraint. We apply adapted velocities whenether the child node is inside the parent. In addition we further seperate if the childnode is already inside an closer radius or not.] 
nodes.forEach(node => {
    // METHOD 3.75: Outer and Inner Radius + jiggle
    let diffX = node.x - parentNode.x;
    ... (same for y,z)

    let layerInt = parseInt(node.layer);

    let velocityDiffX;
    if(Math.abs(diffX) < radius){//child is inside parent
        if (Math.abs(diffX) < innerRadius) {
            velocityDiffX = diffX * jiggle(layerInt * const);
        }else{
            velocityDiffX = diffX * strength / const;
        } 
    }else{//child is outside parent
        velocityDiffX = diffX * strength + jiggle(layerInt);
    }
    ... (same for y,z)

    node.vx -= (velocityDiffX/constant);
    ... (same for y,z)

    node.vx *= (1-alpha);
    ... (same for y,z)
}
\end{lstlisting}

\subsection{Rendering}
\label{sec:rendering}
Goal of Rendering: 
Performance, Support Transparency of nodes. 

Instancing per Links, 
Instancing per Hierarchical Layer of nodes, 
Transparency,
Wireframe

\subsection{VR Navigation}
\label{sec:vrInteractions}

For navigation, we provide two methods free flying and an animated teleport.
Before we go into the details of the technique, it is important to understand the core concept of navigation for 6-DOF VR headsets.\\
The camera is placed in a virtual camera rig which represents the users available movement space (see Figure \ref{fig:vrCameraRig}). 
VR interaction prevent programmatically changing the position of the headset or controllers in the rig. 
The only way the headset and controllers are moved inside the rig is by physical movement in the real world, therefore enable free walking inside the virtual rig.
However, we can programmatically change the position of the rig inside the virtual scene. 
We apply this technique for our free flying and teleport technique.
\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{graphics/vrCameraRig.jpg}
    \caption{2D representation of the camera rig technique in VR. The scene and rig each have their own coordinates. The headset move around the rig by physical movement and the rig moves around the scene by programmatically navigation methods like free flying or teleportation.} 
    \label{fig:vrCameraRig} 
\end{figure}
\\
The free flying technique is already provided by A-Frame and the implementation from the prior paper.
We just added some additional controller button callbacks for manually selecting the fly speed and rotating the rig in the virtual scene.\\
For the animated teleport technique we no longer fly to the center of the selected node instead the target position is the node border barely on the inside of the node. Therefore we added a new calculation for the target position which can be seen in Listing \ref{lst:calculationFlyToNode}, the process is also visualized in Figure \ref{fig:vrFlyToNode}.

\begin{lstlisting}[language=JavaScript,label={lst:calculationFlyToNode},caption=Matrix calculations for determining the target position of the animated teleport.]
//1. get all needed information
const nodePos = node.pos;
const nodeRadius = multilayerNodeDiameter(node.__data);
const sceneScale = detailLayout.getCurrentSceneScale() ;
let cameraPos = getCurrentCameraRigWorldPos().add(getRelativeCameraToRigPos());
//2. calculate the node border position between the camera and nodeCenter
const vecNodeToCamera = cameraPos.add(nodePos.clone().negate());
const vecCenterToNodeBorder = vecNodeToCamera.normalize().multiplyScalar(nodeRadius*0.95*sceneScale);//*0.95 as we want to be slightly inside the selected node
//3. convert position so that the headset instead of the rig is centered
const positionNodeBorder = nodePos.clone().add(vecCenterToNodeBorder);
const positionNodeBorderCorrectedWithCurrentCameraPos = positionNodeBorder.clone().add(getRelativeCameraToRigPos().negate());
//4. initiate animated teleport
flyToPosition(positionNodeBorderCorrectedWithCurrentCameraPos, flyingElement);
\end{lstlisting}
\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{graphics/flyToNodePositionCalc.jpg}
    \caption{2D representation of the calculation for the target position for the animated teleport. A virtual teleport path is formed between the center of the node and position of the headset. This virtual path is extended till it reaches the end of the node.} 
    \label{fig:vrFlyToNode} 
\end{figure}

In a first step we get all information needed including the camera position in world space, position of the node center, the node radius and the current adaptable scale of the scene.
Then we calculate the position of the crossing between the node border and the direct path from the camera to the node center. We do this by calculating a vector from the node center to the camera position. After normalizing the length and applying this vector with the correct length determined by the node radius and scene scale to the center of the node we get the position where the headset should end up when teleporting to the selected node.
We can only manipulate the headset position indirectly through changing the rig position, therefore we have to calculate this position in the third step. 
The resulting position is the final coordinates that we center the rig so that the VR headset ends up barely inside the node.\\
For calculating the target position when flying to the parent Node we apply a similar technique which is shown is Figure \ref{fig:vrFlyToParentNode}.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{graphics/flyToParentNode.jpg}
    \caption{2D representation for calculating the teleporting positing when teleporting to the parent Node.} 
    \label{fig:vrFlyToParentNode} 
\end{figure}

\subsection{Scaling of the virtual scene}
\label{sec:scaling}

In Section \ref{chap:ps-spatialReference} we described why it is necessary to scale up the entire scene while navigating through the graph. 
The challenge while scaling is that the users relative position in comparison the to virtual scene must not change. Otherwise, the user will get distracted while exploring.
When applying a scaling matrix then center of the scene (0,0,0) stays in the same position, all other points are move by the scaling process. 
Therefore, we translate the target position of the animated teleport into the center of the scene, then scale up/down and afterwards translate the scene back.
In order to achieve a smooth user-friendly scaling instead scaling to the desired size once we do an animated scaling. The duration of the animation has to be the exact time as the animated teleportation otherwise the teleportation path will get distorted and not be a straight line anymore.
For manual scaling we use the world position of the headset for translating. Both calculations can be seen in Listing \ref{lst:scaling}.

\begin{lstlisting}[language=JavaScript,label={lst:scaling},caption=Simplified algorithm for calculating the scaling matrix]
const resultScalingMatrix = new THREE.Matrix4();
if(scalingPos === undefined){//manual scaling
    resultScalingMatrix
        .multiply(translateMatrixRig)
        .multiply(translateMatrixCam)
        .multiply(scaleMatrix)
        .multiply(reverseTranslationMatrixCam)
        .multiply(reverseTranslationMatrixRig);
}else{//automated scaling while animated teleportation
    resultScalingMatrix
        .multiply(translationMatrix)
        .multiply(scaleMatrix)
        .multiply(reverseTranslationMatrix);
}
return resultScalingMatrix;
\end{lstlisting}

\subsection{Filtering visible links}
\label{sec:linkFiltering}

Changing the visibility of the links requires either a directly selected node which is accomplishment by ray casting or the most inner node related to the users position.
In order to determine the most inner node we first have to check for every node if the user is inside it or not (see Listing \ref{lst:checkInsideNode}). We store all entered nodes in a temporary node cache in order to calculate the most inner node (see Listing \ref{lst:determineInnestNode}).
Then we update the visibility of all links as described in Section \ref{chap:ps-filterLinks}.

\begin{lstlisting}[language=JavaScript,label={lst:checkInsideNode},caption=Simplified algorithm to determine whenether the user is inside a node or not.]
//called for every frame
for (var i = 0; i < graphData.nodes.length; i++) {
    ...
    var dist = nodePos.distanceTo(getCurrentCameraRigWorldPos().add(getRelativeCameraToRigPos()));
    var threshold = multilayerNodeDiameter(node) * sceneScale;
    if (dist <= threshold) {
        detailLayout.onEnterNode(node);
    }else{
        detailLayout.onLeaveNode(node);
    }
}
\end{lstlisting}

\begin{lstlisting}[language=JavaScript,label={lst:determineInnestNode},caption=Simplified algorithm to determine the inner most node.] 
getCurrentInnestNode(){
    let innestNode = {id:"-",layer:-1};
    for (let enteredNodesKey in this.enteredNodes) {
        if(innestNode.layer < parseInt(this.enteredNodes[enteredNodesKey])){
            innestNode.id = enteredNodesKey;
        }
    }
    return this.nodeCache[innestNode.id];
}
\end{lstlisting}
