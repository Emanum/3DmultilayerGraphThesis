\chapter{Proposed Solution}

\section{Initial Problem}

Our initial motivation for this thesis was to enable visualization of hierarchical multilayer datasets in a VR environment. 
This is necessary because data scientists often deal with these datasets and because of the complexity of the data they often rely on classical 2D visualization software.
However, current 2D approaches quickly reach their limits and run into the problem of visual clutter, especially when visualizing large networks and deep hierarchies. For example the comorbidity network we saw in Figure \ref{fig:original2DdiseaseNet} hides links and first and foremost is limited to only one hierarchical layer. This is not a problem for this dataset in particular, but we wanted a solution to visualize n hierarchical networks.
To improve process of hierarchical network exploration, we wanted to use the capabilities of VR and 3D information visualization. 

\section{Requirements}
\label{chap:ps-requirements}
We defined multiple requirements based on the problems we described in the previous Section:\\
\begin{enumerate}
    \item[R1]\label{req:R1} The visualization is able to visualize $n$ hierarchical network datasets. Each node represents a network for itself. Links are possible within nodes of the same network but also to nodes of different networks with the same hierarchical depth.
    \item[R2]\label{req:R2} The visualization can display larger networks than classical 2D approaches, before reaching a critical point of visual clutter where a meaningful exploration is not possible anymore.
    \item[R3]\label{req:R3} The visualization provides a room scale experience for small and large room sizes.
    \item[R4]\label{req:R4} The visualization fully utilized the tracking capabilities of the VR Headset to optimize interaction experience for the user.
    \item[R5]\label{req:R5} The visualization allows a flexible navigation through the graph during the exploration process.
    \item[R6]\label{req:R6} The visualization ensures the clarity of the entire visualization by applying appropriated techniques according to Shneiderman's visualization seeking mantra.
    \item[R7]\label{req:R7} The visualization ensures that deep hierarchical networks are equally represented and can be explored intuitively.
    \item[R8]\label{req:R8} The visualization uses the HTC Vive as a target platform, but the concepts should be transferable to other 6-DOF Headsets. 
\end{enumerate}

\section{Layout}
\label{chap:ps-layout}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth, trim={0cm 0cm 0cm 3.5cm},clip]{graphics/filterLinks/allLinks.jpg}
    \caption{A sketch of our preferred layout in 2D. Instead of 2D circle we use 3D spheres in the real application. The number before the dot represents the depth respectively the layer of the node and the number after the dot a node id unique for each layer.} 
    \label{fig:layoutSketch} 
\end{figure}
To achieve \hyperref[req:R1]{R1}, we created our own layout. The core concept is to combine the nested technique of tree maps with multilayer techniques. However instead of flat surfaces as seen in many multilayer network visualizations we use three-dimensional spheres. 
By using the third dimension in combination we can achieve a better distribution of nodes and links and therefore display larger networks than with classical 2D approaches, as required in \hyperref[req:R2]{R2}. 
To dynamically build this layout for any given data as input we developed a custom force based system.
The main goal is to prevent overlapping of nodes while still visualize the different clusters of hierarchical nodes by proximity. Figure \ref{fig:layoutSketch} shows a sketch of our envisioned layout. The entire data can be seen as a tree structure where each entity in the tree is a graph itself nested in its parent node. Therefore, the number of nodes can grow exponentially with the number of hierarchical layers. Note that the root node is not displayed because it would just create an additional complexity in the visualization without adding any meaningful benefit.

\subsection{Layout Forces and Constraints}
\label{chap:ps-forces}
The force system consists of multiple forces and constrains to automatically calculate the positions of all nodes. We want to achieve an evenly distributed graph while still fulfilling the rules we set ourselves for hierarchical nesting. Firstly we define a set of force and constraint templates:
\begin{itemize}
    \item ManyBody-Force: This force is a repulsion force and causes nearby nodes to push each other off. The strength decreases with the distance of the nodes. This allows the distribution of nodes evenly among the available space.
    \item Link-Force: It is the counterpart of the ManyBody-Force and pulls nodes connected via a link closer together. Together with the ManyBody-Force it enables us to model a distributed graph while still clustering connected nodes and minimizing the chance for a link to cross a not involved node.
    \item Collision-Force: This force is basically a reinforced version of the ManyBody-Force it prevents nodes from overlapping in the case the ManyBody- and Link-Force push nodes into each other.
    \item Spherical-Constraint: The spherical-constraint is the core concept of our layout we use for nesting nodes. It allows us to “squish” multiple nodes in a sphere for any given point and radius. The center of the sphere can variate for each simulation step. The constraint works by constantly adding a slightly randomized velocity for each node towards the center of the sphere. If the node is outside the sphere, then the velocity is increased drastically.
    \item Center-Force: This force helps us to position the entire visualization in the center of our viewpoint, however there is no strict distance or radius like the Spherical-Constraint applies.
\end{itemize}

It is important to understand that these templates are not applied equally to all nodes. Instead, we apply multiple instances of these forces with different parameters to subsets of our graph. 
This separation allows us to prevent of interference between different group of nodes for different parents.\\ 
Firstly we distinguish between the top hierarchical layer, from now on called layer $0$, and all other subsequent layers 1,2,3 and so on. Each node and link instance is assigned their respectively layer attribute. Layer $0$ is treated separately because these nodes have no direct assigned parent node. 
For Layer $0$, the center-force with the coordinates $(x:0,y:0,z:0)$, ManyBody-Force, Collision-Force and lastly Link-Force are applied to all nodes and links with a layer $0$ attribute. 
As for layer $1$ to layer $n$, we do not apply forces by layer but instead by parent node. For each parent we add a Collision-force, ManyBody-force and Link-force for all child nodes and links. In addition, the Spherical-Constraint is also added for all child nodes with the position of the parent node as a center. Note that the position of the parent node can change each simulation iteration, so we also update the center position for that specific constraint entity.\\
In conclusion, the total number of forces and constraints in our force system is: 
\begin{equation}
    |forces| + |constraints| \: = 4 \, + 4 * |parent\_nodes|
\end{equation}

\subsection{Stability of the force system}

A big challenge in our force system was to equally balance out all added forces so that each one performs their specific task and does not influence the effect of other forces. To achieve that, we parameterized each force with a strength parameter.
In addition, we also use the concept of an alpha Target from D3.js \cite{bostock_d3js_nodate}. To put it briefly, it is an additional value which decreases throughout the simulation this allows the simulation to “cool down” and stop it as soon as it reaches that set alpha Target value. We use the alpha Target to control the number of simulation interations. 
Besides balancing out the different templates of forces we also have to decrease the strength recursively for each layer as the nodes and their radius get smaller each layer iteration.\\
To further improve the stability of the layout, we add a small amount of randomness to the applied velocities and strengths. This improves the nodes distribution and prevent scenarios where multiple nodes receive the same forces and therefore overlap each other.\\
During our optimization we stumbled upon the problem that nodes tend to jump rapidly to a far position. This is only natural as we squish the nodes into the sphere of the parent while still applying the ManyBody- and Collision-Forces. Therefore, in some boundary scenarios the only “free” position for this node is further away which results in the jumping behavior. However, it defeats the concept of successively fine-tuning the positions throughout the simulation steps. For example nodes of layer 1-3 would be already positioned well but in the last simulation step the parent node in layer 0 would jump and therefore layer 1-3 nodes are positioned outside the parent layer 0 node again. To circumvent that problem, we do not perform the positioning of all nodes throughout the entire simulation, instead we split up the amount of simulation steps and perform them for each layer successively (see Figure \ref{fig:SimulationSteps}). Beginning with positioning the layer 0 nodes afterwards layer 1 nodes and so on. Luckily, we also gain a performance benefit through that strategy. The reason for this is that the number of nodes grows exponentially for each layer, now that we only perform a subset of the simulation interactions to that increasing number of nodes the sum of position update operations reduces significantly.
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth, trim={5cm 5cm 0 25cm},clip]{graphics/simulationStepsSplit.jpg}
    \caption[Optional caption for the figure list (often used to abbreviate long captions)]{Distribution of all simulation steps for a dataset with 4 hierarchical layers. Each layer receives the same amount of simulation steps, in this case 25 from 100.} % Remove the [...] argument if the original caption should be used in the figure list.
    \label{fig:SimulationSteps} 
  \end{figure}
\section{Graph Representation}
\label{chap:ps-graphRepresentation}
\begin{figure}[!htb]
    \centering
    \includegraphics[width=1\textwidth]{graphics/screenshotNesting.jpg}
    \caption{Overview perspective of the visualization. Each node is rendered semi transparent, this allows the user to detect the topology of the data.}
    \label{fig:ps_nestedLayout}
\end{figure}
\begin{figure}[!htb]
    \centering
    \includegraphics[width=1\textwidth]{graphics/screenshotNestingAndWireframe.jpg}
    \caption{Detail perspective inside of node 0.2. To be able to tell the border of the nodes the node is rendered as a wireframe when looking inside out.}
    \label{fig:ps_wireframe}
\end{figure}
The goal of the graph representation was to choose appropriate rendering methods which allow us to reduce visual clutter and improve clarity while still displaying as much data as possible.\\
To represent the graph data, we choose spheres for nodes, tubes as links and cones as arrow heads indicating the link direction.
With solid textures the overview ability of the visualization would be poor since our layout approach use nested layers and therefore the user could only see one layer of nodes at the same time. To circumvent that we render all nodes transparent, then the user is able to see the nested nodes inside its parent node (see Figure \ref{fig:ps_nestedLayout}). 
However, links inside other nodes are not visible because displaying a huge number of links would rather increase visual clutter and reduce overview. To deal with the big amount of links we apply filter conditions which are described in Section \ref{chap:ps-filterLinks}.
During the entire visualization process that the user should be able to detect the borders of the nodes and their hierarchical nesting.\\ Now that our nodes are transparent, it would be hard to detect if another node is nested in the current parent node the camera is placed into or not. Therefore, we render a wireframe (see Figure \ref{fig:ps_wireframe}) with the help of the improved spatial impression given by the virtual reality room scale experience this allows the user to better see the border of node. To further improve the assignability of nodes to their layers each node and link of the same layer shares a predefined color.\\
Of course an important part of the visualization process is to display names and details for nodes. To this end, each node instance has a billboard text label which is placed on the border of the node and always points to the camera.

To ensure an equal down scaling of all rendered entities as well as correcting the needed node radius for the layout forces we use a visualization wide scaling factor:
\begin{equation}
    scalingFactor = \frac{1}{(layer+1)^{(layer+1)} * constant}
\end{equation}

We also take the number of child nodes into account when calculating the node size: 

\begin{equation}
    nodeSize = baseSize + (numChildNodes * constant)
\end{equation}

\section{Interaction}
\label{chap:solution-interaction}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{chapters/graphics/controllerMapping.jpg}
    \caption{(TODO redo sketch)Controller mapping.} 
    \label{fig:controllerMapping} 
\end{figure}
For \hyperref[req:R4]{R4} we use the 6 DOF tracking of the headset and controllers from the HTC Vive. Therefore, we implemented intuitively interaction techniques to allow teleportation navigation throughout the virtual scene (see Section \ref{chap:solution-navigation}). In addition, the user should be able  to switch the visibility filter between multiple groups of links (see Section \ref{chap:ps-filterLinks}).
What both interactions have in common is that the user has to select a specific node before these actions can be triggered. 
This concludes that our visualization needs a general way to support the selection of nodes which then can be used by various other methods we apply during the exploration process. 
In addition, we also stated in our requirements that we strive to use the tracking capabilities of VR to create intuitively interaction methods.
Therefore, all user interactions can only be provided by the native VR supported hardware, in particular from 6-DOF tracked controllers. 
To this end we implemented the common concept of ray cast selection for selecting nodes in the virtual scene. Along the length of the right controller a ray is cast through the scene. To better visualize the effect, we render a straight red line to imitate a virtual laser pointer (TODO figure). The first intersected node is then used as the selected node for other methods like filtering or teleportation. The id and name attribute of the selected node is displayed in a HUD element centered in the lower part of the screen inside the headset (TODO figure). In addition, the selected node is visually highlighted through graying out the other nodes to increase the contrast (TODO figure). This allows the users to quickly see which node they currently have selected.

Now that the users can select specific nodes in the scene we also have to give them the opportunity to trigger certain actions. These actions include the teleportation navigation (see Section \ref{chap:solution-navigation}), link visibility filter (see Section \ref{chap:ps-filterLinks}). 
Other actions do not require the selection of a node like manual rotation, free flying with an adjustable flying speed and teleportation to the upper layer (see Section \ref{chap:solution-navigation}) as well as manual scaling (see Section \ref{chap:ps-spatialReference}).\\
However, we have to assign each of these actions to a button on the controller (see Figure \ref{fig:controllerMapping}). In order to optimize the intuitive workflow we extend the normal trackpad (nr. 2) click by the position of clicking. 
This allows us to have 5 virtual buttons on the trackpad: top, bottom, left, right and center. The advantage of this method is that these virtual buttons can be used fluently while also interacting with the touch sensitive trackpad at the same time by one finger. 
Therefore, the users can control the free flying direction and speed with their left thumb, as well as triggering the teleportation to the upper hierarchical layer. The right controller acts as a virtual laser pointer, the two actions which need a node selection beforehand can be started by the menu button (nr. 1) and trigger (nr. 7). 
On the right trackpad the scaling and rotation can be controlled.

We optimized our application for the HTC Vive therefore all described button mappings refer to the HTC Vive controllers. However, we only use simple buttons, trackpads and industry standard tracking methods. 
As other 6-DOF headsets usually provide the same or similar interaction possibilities, the interaction concept of our visualization can be applied to other headsets as well.\\

\subsection{Filtering Link Visibility}
\label{chap:ps-filterLinks}

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.6\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{graphics/filterLinks/allLinks.jpg}
        \subcaption{All links represented in the data.}
        \label{fig:linkFilter-all}
    \end{subfigure}
    \begin{subfigure}[b]{0.6\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{graphics/filterLinks/outside.jpg}
        \subcaption{Visible links when the user is outside every node.}
        \label{fig:linkFilter-outside}
    \end{subfigure}
    \begin{subfigure}[b]{0.6\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{graphics/filterLinks/layer0.jpg}
        \subcaption{Visible links when the user inside node 0.0 or selected it by the laser pointer interaction.}
        \label{fig:linkFilter-layer1}
    \end{subfigure}
    \begin{subfigure}[b]{0.6\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{graphics/filterLinks/layer1.jpg}
        \subcaption{Visible links when the user inside node 1.1 or selected it by the laser pointer interaction.}
        \label{fig:linkFilter-layer2}
    \end{subfigure}
    \caption[Optional caption for the figure list (often used to abbreviate long captions)]{Our filtering technique visualized on a small example graph.} % Remove the [...] argument if the original caption should be used in the figure list.
    \label{fig:intro} 
  \end{figure}

To ensure the clarity of the visualization (see \hyperref[req:R6]{R6}) and display larger networks than 2D approaches (see \hyperref[req:R2]{R2}) we apply a filtering approach for links according to the requirements. 
The goal is to reduce visual clutter while providing as much information as possible. 
As a quick recap: Links in our visualization can exist between the same hierarchical parent node but also to nodes with a different parent node as long as the hierarchical depth is the same, Figure \ref{fig:linkFilter-all} shows a small example of all possible link combinations for three hierarchical layers.
Displaying all link at the same time would quickly lead to visual clutter where a meaningful exploration is not possible anymore. 
Therefore, we came up with our own filtering logic which decides which links are displayed and which are not.\\ 
The filter uses a specific node this can either a directed selected node from the user by the laser pointer interaction or indirectly selected through the position of the user. A directly selected node always overrules the indirectly selected.

\begin{itemize}
    \item If no node is selected and the user is not inside any node only the links from the top layer 0 are displayed (see Figure \ref{fig:linkFilter-outside}).
    \item All links with source or target of that selected node are displayed. 
    \item All links with source or target of direct child nodes from the selected nodes are displayed. Direct child here means the hierarchical child one layer below the selected node. 
    \item All other links are not visible.
\end{itemize}

As an example, Figure \ref{fig:linkFilter-layer1} shows a situation where node 0.0 is selected. Therefore, all links connecting direct nodes of 0.0 and links from 0.0 itself are displayed. However, links of deeper nested nodes links 2.1 are not displayed. 
Another situation can be seen in Figure \ref{fig:linkFilter-layer2}, here only links from node 1.1 and 2.4 are visible but the links from node 1.2 and 0.0 are not visible anymore.\\
In an earlier version we not only displayed the links of direct child nodes but instead links of all recursively child nodes, e.g. in Figure \ref{fig:linkFilter-layer1} the green links would also be visible. However, this quickly leads to visual clutter therefore we stuck to the direct child approach.

In order to support an intuitively and flexible exploration workflow the user also has the ability to lock a currently applied filter.
With the left menu button (see Figure \ref{fig:controllerMapping}; nr 1) the state of the locked can be toggled. 
If the lock is active, the indirect selecting of nodes by the position of the user is disabled. The lock is also automatically set if a node is directly selected by the laser pointer. 
This allows the user to freely navigate around the visualization without constantly having to deal with changing link visibility. 
On freeing the lock the indirect select automatically kicks in again and select the current node the user is position in.

\section{Navigation}
\label{chap:solution-navigation}
In order to fulfill \hyperref[req:R3]{R3} we support a walkable room scale VR experience setup to navigate in the virtual scene. As we stated in Section \ref{chap:rw-vrnavigation} walking provides the best immersive and intuitive navigation method.
However, the graph will usually be larger than the users available space, to this end we also include other navigation methods.
To allow a maximum of flexibility during the navigation, as required for \hyperref[req:R5]{R5}, we provide two navigation methods: animated teleporting for covering larger distances and free flying to perform small and precise position adjustments.

The user should be able to move to other far away parts of the graph without loosing orientation or interrupt the exploration flow. A common example is to jump to a connected node of another parent node. 
Therefore, we implemented an animated teleportation method. The user can select a node by the laser pointer and then press the right trigger (nr. 7) to initiate the teleportation. The target position is the closest edge barely inside the node. This allows the user to get an overview of all nodes and links in that selected node.   
As with a simple teleportation over long distances, the user might lose the orientation, we perform a short animation to the target position. The speed of the animation is done with an ease-in and ease-out transition.
Remember that as soon as the users enters another node, the visibility of links changes, provided there is no active lock.\\
Instead of teleporting deeper into the hierarchical networks, the user can also teleport to the parent hierarchical layer. In particular to the edge barely outside the node the user is currently located in, this teleportation can be triggered by pressing the middle of the left trackpad.\\
All teleportation methods can be freely combined even during the animation, therefore allowing the user to fluently move around the graph. 
To further improve the overview, we display the current hierarchical layer as a text element floating over the right controller (see Figure (TODO)). 

To perform small adjustments in position, especially when the users free space is limited, the user can freely fly in the virtual scene. To control the direction, the left touch sensitive trackpad can be used. 
The forward direction is linked to the users gaze direction, so the final fly direction is the combination of gaze direction and trackpad touch position. 
In addition, the fly speed is adjustable with a click on top or bottom of the trackpad.    
Similar to other VR applications we also implemented rotation of the entire room scale space in the virtual scene. This allows the users to better position themselves in the real world. 
Particular with cable bound headsets this is useful because multiple rotations in the real world require to step over the cable. In other VR applications the movement is usually done with the left controller and rotation with the right, analogue to Gamepad controls in most games. Therefore, we mapped the rotation to the right trackpad by clicking either on the left or right corner.

\subsection{Challenge of spatial reference in VR}
\label{chap:ps-spatialReference}
In \hyperref[req:R7]{R7} we stated that the visualization should ensure an equal representation of deep hierarchical networks. The challenge is that nodes and links are getting exponentially smaller with each hierarchy step.
This introduces a new problem as after a few iterations the nodes and links in the virtual scene are too small to recognize.
2D visualizations usually deal with that by adjusting the viewport of the visualization e.g. zoom in and out. 
However, in the context of a 3D VR visualization a simple zooming approach is not possible. 
In a 3D non VR visualization that problem can be mitigated by adjusting the movement speed. Therefore, distances seem similar in size for the inner nodes and navigation is also possible without overshooting the target position.\\ 
In VR the situation is trickier due to the spatial impression by rendering two perspectives. On a 2D projected image, like on a normal computer screen distances in the scene can only be estimated through movement. From a static image the human eye is not able to recognize the distance or size of an object without any additional size reference. 
In the real world however we constantly see objects from two different perspectives as we have two eyes, this enables the human mind to estimate distances without the need of any movement. VR works the same way by rendering the scene from two different perspectives each for one eye.  
This means that a simple trick of adjusting the movement speed is not working for our VR visualization. (die erklärung ist ein bißchen umständlich, Hast du vorschläge wie man das besser formulieren könnte? Finde die Problematik recht schwer zu beschreiben)
That problem makes itself noticeable for example as layer 4 nodes in VR are the size of a finger where layer 0 nodes are the size of the entire room.
In addition to the perspective problem changing the movement speed also does not work because movement in a 6-DOF VR headset is also always done by movement in the real world and obviously the physical movement speed can not be adjusted.

The only real solution to that problem is scaling the entire scene. Therefore, we implemented two techniques in our visualization:
\begin{itemize}
    \item Dynamically adjust the fly speed while navigating in the graph. On entering a node the speed slows down, when leaving it speeds up again. 
    \item Dynamically scaling the entire scene. When teleporting to a node in a deeper hierarchy layer, an upscaling transition is started. On teleporting to the parent layer a downscaling transition is started. 
\end{itemize} 

In addition, the scale can also be manually changed by the user at any time by pressing the upper or lower corner of the right trackpad. 
In \hyperref[req:R3]{R3} we stated to support small and large room sizes.  However, the scale can not be optimized beforehand for a walking experience. The manual scaling gives the users the ability to adjust the scale to their available space and personal preference.
\section{Exploration Flow}
\label{chap:ps-explorationFlow}
To provide an optimal exploration experience of the hierarchical network, we use the overview and detail concept of the visual information seeking mantra \ref{seeking mantra} as stated in \hyperref[req:R6]{R6}. 
At the beginning the user is in the overview perspective. He/she is placed on a further position away from the center where a good look on the entire graph is possible, in addition rotation to the graph can be applied.
When the user decides to dive deeper into the network, a node can be selected via the ray cast controller interaction. This triggers the transition into the detail perspective. 
Here all navigation and interaction methods described in Section \ref{chap:solution-interaction} and \ref{chap:solution-navigation}
are available. 
A core concept for the exploration in the detail perspective is the use of the VR room-scale navigation experience. As the user is able to tweak the scale of the scene to their liking, walking around the graph is possible for different room sizes. The goal is to improve spatial impression of the graph which results in a better clarity and understanding of the data.

TODO abschluss vom chapter